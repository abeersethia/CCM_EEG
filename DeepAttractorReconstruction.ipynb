{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd9dc8d-5a9a-4b60-87e8-b00be5e93ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchdiffeq import odeint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0e3244-8655-48c6-b8fd-58ac2034b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz_system(state, t, sigma=10., beta=8./3, rho=28.):\n",
    "    x, y, z = state\n",
    "    dx = sigma * (y - x)\n",
    "    dy = x * (rho - z) - y\n",
    "    dz = x * y - beta * z\n",
    "    return np.array([dx, dy, dz])\n",
    "\n",
    "def simulate_lorenz(initial_state, timesteps, dt):\n",
    "    from scipy.integrate import odeint as scipy_odeint\n",
    "    t = np.linspace(0, timesteps*dt, timesteps)\n",
    "    return scipy_odeint(lorenz_system, initial_state, t), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f981c0cb-0e93-4bcc-a7cc-a6edac89a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "initial = [1.0, 1.0, 1.0]\n",
    "data, t = simulate_lorenz(initial, 10000, 0.01)\n",
    "x = data[:, 0]  # 1D observation\n",
    "\n",
    "# Create sliding windows\n",
    "def create_sequences(x, window_size):\n",
    "    X = []\n",
    "    for i in range(len(x) - window_size):\n",
    "        X.append(x[i:i+window_size])\n",
    "    return torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "window_size = 50\n",
    "seq_data = create_sequences(x, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af38ef6-0d1a-455f-a05b-e39a48319559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Latent Dynamics as Neural ODE\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, latent_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        return self.net(z)\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.fc(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b14ced9-b4fe-4de2-9df7-596fb4861c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full model\n",
    "latent_dim = 3\n",
    "encoder = Encoder(window_size, latent_dim)\n",
    "odemodel = ODEFunc(latent_dim)\n",
    "decoder = Decoder(latent_dim, 1)\n",
    "\n",
    "params = list(encoder.parameters()) + list(odemodel.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f52bd-e219-4240-831e-e5d272419234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 9.788390\n",
      "Epoch 2, Loss: 4.963071\n",
      "Epoch 3, Loss: 5.272738\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 10\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(seq_data) - 1):\n",
    "        x_seq = seq_data[i]\n",
    "        x_next = seq_data[i + 1][-1:]  # predict the next point\n",
    "\n",
    "        z0 = encoder(x_seq)\n",
    "        t_pred = torch.tensor([0., 1.], dtype=torch.float32)\n",
    "        z_pred = odeint(odemodel, z0, t_pred)[-1]\n",
    "        x_pred = decoder(z_pred)\n",
    "\n",
    "        loss = loss_fn(x_pred, x_next)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(seq_data):.6f}\")\n",
    "\n",
    "# Visualize latent space\n",
    "z_latent = encoder(seq_data[:1000]).detach().numpy()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot(z_latent[:, 0], z_latent[:, 1], z_latent[:, 2])\n",
    "ax.set_title(\"Latent Space Trajectory\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb29c5b1-6a65-4014-954e-67c0b3a525cc",
   "metadata": {},
   "source": [
    "# Conclusion: Interpretation of Latent Space Trajectory\n",
    "\n",
    "The plot above visualizes the trajectory of a data sequence in a learned 3D latent space. \n",
    "Each point on the curve represents a latent vector at a given timestep or sequence position, \n",
    "mapped from the original high-dimensional input (e.g., a frame, word, or state) into a compressed \n",
    "latent representation via an encoder or similar transformation.\n",
    "\n",
    "Key observations:\n",
    "\n",
    "1. The trajectory is continuous and smooth, indicating that the model has learned \n",
    "   structured latent representations rather than random noise.\n",
    "\n",
    "2. The presence of curves and loops suggests that the model captures some temporal \n",
    "   or spatial patterns, typical in sequential data (e.g., time series, text, or video).\n",
    "\n",
    "3. The scale across axes is not uniform (especially along the Y-axis), which may indicate:\n",
    "   - Uneven variance in latent dimensions.\n",
    "   - A need for normalisation or regularisation.\n",
    "   - Or possibly a plotting issue (e.g., incorrect axis selection).\n",
    "\n",
    "4. A sharp transition at the start may reflect a sudden change in input features or \n",
    "   initial model instability during encoding.\n",
    "\n",
    "Overall, this trajectory provides insight into how the model internally represents \n",
    "the progression of input data. Further analysis or dimensionality reduction \n",
    "(e.g., PCA, t-SNE) could be used to refine interpretability and check clustering behaviour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f336d9f-76ce-48ba-876e-e9197e86c234",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281781be-1c98-4261-8a76-33e753ced4ee",
   "metadata": {},
   "source": [
    "### Lorenz Attractor Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564c9dc-92d6-4d57-9e91-756a1671ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lorenz system differential equations\n",
    "def lorenz(t, state, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "    x, y, z = state\n",
    "    dx_dt = sigma * (y - x)\n",
    "    dy_dt = x * (rho - z) - y\n",
    "    dz_dt = x * y - beta * z\n",
    "    return [dx_dt, dy_dt, dz_dt]\n",
    "\n",
    "# Initial conditions and time span\n",
    "initial_state = [1.0, 1.0, 1.0]\n",
    "t_span = (0, 40)\n",
    "t_eval = np.linspace(t_span[0], t_span[1], 10000)\n",
    "\n",
    "# Integrate the Lorenz equations\n",
    "solution = solve_ivp(lorenz, t_span, initial_state, t_eval=t_eval)\n",
    "\n",
    "# Plotting the Lorenz attractor\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(solution.y[0], solution.y[1], solution.y[2], lw=0.5)\n",
    "ax.set_title(\"Lorenz Attractor\")\n",
    "ax.set_xlabel(\"X Axis\")\n",
    "ax.set_ylabel(\"Y Axis\")\n",
    "ax.set_zlabel(\"Z Axis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941af14-dd77-43ce-9805-eb57f47ad020",
   "metadata": {},
   "source": [
    "## ðŸ§  Conclusion\n",
    "\n",
    "The **Lorenz attractor** is a classic example of a chaotic system. Even though it's defined by simple deterministic equations, it produces highly complex and non-repeating behavior.\n",
    "\n",
    "This sensitivity to initial conditions â€” where tiny differences lead to vastly different outcomes â€” is a hallmark of **chaos theory**.\n",
    "\n",
    "In this simulation, we used the standard Lorenz parameters:\n",
    "- **Ïƒ (sigma)** = 10\n",
    "- **Ï (rho)** = 28\n",
    "- **Î² (beta)** = 8/3\n",
    "\n",
    "These values create the iconic \"butterfly\" or double spiral pattern in 3D space.\n",
    "\n",
    "> This visualization is more than just visually interesting â€” it's foundational in the study of **nonlinear dynamics**, **weather prediction**, and **chaotic systems** in physics and beyond.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
